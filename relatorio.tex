\documentclass[12pt,a4paper]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage[brazilian]{babel}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{hyperref}

\newtheorem{thm}{Teorema}
\newtheorem{lemma}[thm]{Lema}

\title{MC458 - Projeto}
\author{
        Juca Magalhães Meniconi\\
        \texttt{RA: 281803}
        \and
        Lucas Beserra Fernandes\\
        \texttt{RA: 281815}
        \and
        Vinícius Augusto Silva Brasileiro\\
        \texttt{RA: 247391}
}
\date{}
\usepackage{caption}
\begin{document}
    \maketitle

    Nesse projeto, exploraremos duas estruturas de dados que representam matrizes esparsas. Iremos analisar operações e características de cada estrutura, a fim de verificar a eficiência dessas estruturas. Com isso, discutiremos as vantagens e desvantagens das implementações quando comparadas entre si e quando comparadas com representações de matrizes não esparsas. O código está disponível em \url{https://github.com/viniasbr/MC458-projeto}
    
    \section{Descrição das Estruturas}
    \subsection{Estrutura 1 - Hash Table}
    A Estrutura 1 é uma representação de matrizes esparsas baseada em Hash. Armazenaremos todos os elementos não nulos da matriz em um vetor. O local em que cada elemento é armazenado é baseado em um Hash no número da linha e da coluna. Para lidar com possíveis colisões, cada posição do vetor é uma lista ligada.  O tamanho do vetor é atualizado dinamicamente para manter o Load Factor entre 0.25 e 0.75, o que é feito junto a uma chamada de \texttt{resize}.
    \subsection{Estrutura 2 - AVL}
    A Estrutura 2 é uma representação de matrizes esparsas baseada em árvores, mais especificamente em árvores AVL. Teremos duas estruturas que fazem parte da matriz: uma árvore interna e uma externa. Cada árvore obedecerá à propriedade de árvore de busca de acordo com o número da linha ou número da coluna. A árvore externa terá como dado o endereço de uma matriz interna, que representa todos os elementos não nulos da matriz que estão na linha/coluna indicada pela árvore externa. A árvore interna terá o número da coluna/linha de cada elemento, assim como o valor do elemento da matriz, que sabemos que está na posição indicada pelos índices das matrizes externa e interna. 
    \section{Análise das Complexidades}
    \subsection{Estrutura 1 - Hash Table}
    Para a Estrutura 1, analisaremos a complexidade esperada. O custo de alocação de memória é tido como constante.
    \subsubsection{Complexidade de espaço de \texttt{HashMatrix}}
    A estrutura \texttt{HashMatrix} possui 5 campos. Três deles são inteiros, um é um booleano e o último é um ponteiro para um vetor de ponteiros para listas ligadas. O tamanho do vetor de espalhamento é dinâmico, e o como o número de elementos na lista ligada de cada posição do vetor também é variável, o tamanho de cada posição do vetor também é. Porém, sabemos que o número de elementos na matriz esparsa é $k$. Sabemos também que o Load Factor $\alpha$ está sempre entre 0.25 e 0.75, ou seja, o tamanho do vetor $m$ está entre $\frac{4k}{3}$ e $4k$. Portanto, o tamanho do vetor é $O(k)$. Cada posição do vetor aponta para uma lista ligada que armazena os elementos que caíram naquele bucket. No pior caso, todos os $k$ elementos caem no mesmo bucket, e a lista ligada associada a esse bucket terá tamanho $k$. Mas como sabemos que o número total de nós entre todas as listas ligadas do vetor é $k$, sabemos que mesmo em uma distribuição improvável, que não corresponde à esperança de nós por bucket, só teremos o tamanho dos nós do bucket, e de ponteiros para bucket. Ambos lineares em $k$. Assim, o espaço total gasto pela estrutura \texttt{HashMatrix} é $M(k) \in O(k)$.
    \subsubsection{Complexidade de \texttt{hash}}
    A operação de Hash inclui apenas um número constante de operações básicas. Assim, essa função $T(k) \in \Theta(1)$. Porém, analisaremos também a esperança de elementos por bucket, que será usada para a análise da complexidade do caso médio de outras operações. Como nesse hash a tabela de espalhamento é um vetor, eventualmente será chamada de vetor de espalhamento ou simplesmente de vetor. 

    Vamos presumir que cada par linha coluna $p$ que fazemos o hash tem igual probabilidade de cair em cada bucket em um vetor de tamanho $m$. A probabilidade de $p$ cair no i-ésimo bucket é $\frac{1}{m}$. Se temos $n$ elementos nesse vetor, então a esperança do número de elementos em qualquer bucket é $\frac{n}{m}$. Note que $n$ e $m$ tem uma relação limitada pelo load factor $\alpha$ do hash. Como o aumento ou diminuição do load factor só pode ser feito pela função \texttt{set\_element\_hash}, e essa função ajusta o tamanho do vetor de espalhamento sempre que o número de elementos sai do intervalo aceitável [0.25, 0.75], então 
    $$0.25 <\frac{n}{m}<0.75$$
    Portanto, a esperança do número de elementos por bucket  $T  (k) \in O(1)$.

    
    \subsubsection{Complexidade de \texttt{createHashMatrix}}
    Nessa operação, alocamos memória de acordo com o Struct \texttt{HashMatrix}, o que tem custo assumido como constante. Então, atribuímos valores iniciais para os 5 campos da Struct, cada atribuição em tempo constante. Então retornamos a matriz. $T  (k) \in O(1)$. 
    \subsubsection{Complexidade de \texttt{resize}}
    Após algumas operações de custo constante, percorremos o vetor de espalhamento, e para cada elemento do vetor, percorremos o bucket associado a ele. Como sabemos que, em média, teremos um número constante de elementos por bucket, a classe assintótica é igual ao de só percorrer o vetor.  Para cada elemento do vetor, faremos uma quantidade constante de operações. Além disso, sabemos, pelo número constante de elementos por bucket e pelo limite de Load Factor entre constantes, que o número de elementos no vetor de espalhamento é igual ao número de elementos na matriz multiplicado por uma constante. Portanto, o custo esperado de \texttt{resize} é $T(k) \in O(k)$.
    \subsubsection{Complexidade de \texttt{get\_element\_hash}}
    Nessa função, fazemos um número constante de operações, então executamos a função \texttt{hash}, de custo constante. Então, percorremos o bucket da posição indicada pelo \texttt{hash}. Como já foi discutido, o número esperado de elementos por bucket é constante, portanto, o custo esperado de toda operação é constante, e $T(k) \in O(1)$.
    \subsubsection{Complexidade de \texttt{set\_element\_hash}}
    O custo esperado da operação \texttt{set\_element\_hash} é parecido ao de \texttt{get\_element\_hash}. Fazemos um número constante de operações, com exceção da iteração do bucket, que tem número esperado de elementos constante. Portanto, $T(k) \in \Theta(1)$. 
    Essa operação também serve para remover elementos caso seja atualizado um elemento não nulo com 0, tornando-o nulo. Nesse caso, o custo assintótico não muda.
    A diferença é que a função pode chamar \texttt{resize} eventualmente, o que faria com que o custo fosse $O(k)$. Porém, vamos verificar o custo amortizado.

    Entendemos como \textit{Load Factor} $\alpha$ a razão entre o número $n$ de elementos no vetor e o número $m$ do tamanho do vetor. Ou seja:
    $$\alpha = \frac{n}{m}$$
    Se o hash está entre o $\alpha \_min$ $\alpha \_max$, não fazemos nada. Porém, se o Load Factor ficaria além do intervalo, alteramos o tamanho da tabela com \texttt{resize}. Se o $\alpha$ está perto do limite superior, dobramos o tamanho. Se $\alpha$ está perto do limite inferior, então cortamos o tamanho pela metade. Ou seja, estamos sempre dentro dos limites estipulados para o Load Factor. Sabemos que o custo de \texttt{resize}, como demostrado abaixo, $T(k) \in O(k)$. Porém, em uma sequência sucessiva de inserções, podemos calcular o custo médio por inserção porque sabemos que não é toda inserção que irá levar a um chamado de \texttt{resize}. 

    Considere uma sequência de n inserções. Cada \texttt{resize} custa O(i), sendo i o tamanho atual. O número de inserções que podemos fazer entre cada \texttt{resize} é $\Theta(i)$ porque dobramos o tamanho a cada vez. Portanto, se fazemos uma operação O(i) a cada $\Theta(i)$ inserções, então o custo amortizado da inserção é $O(1 + \frac{O(i)}{\Theta(i)})$, portanto o custo amortizado da inserção $\in O(1)$.
    
    \subsubsection{Complexidade de \texttt{matrix\_multiplication\_hash}}
    Nessa operação, criamos uma matriz resultado (custo constante). Então percorremos o vetor de espalhamento para a matriz $A$, e então percorremos os buckets do vetor de $A$. Para cada elemento no bucket, percorremos o vetor de espalhamento de $B$, e para cada elemento no vetor de espalhamento de $B$, percorremos o bucket associado. Apesar de ser um laço quadruplamente aninhado, temos limitantes garantidos para o tamanho do vetor com relação a $K$, e temos limitantes esperados do número de elementos por bucket. Além de sabermos que, em todos os buckets de uma matriz $M$, temos exatamente $Km$ elementos. Como já discutido antes, o custo esperado de percorrer o vetor de espalhamento, mesmo se também percorremos os buckets de cada elemento, é linear em $K$. Como percorremos $A$, e $B$ de maneira aninhada, o custo esperado é $T(k) \in O(Ka\cdot Kb)$, sendo $Ka$ o número de elementos na matriz esparsa $A$ e $Kb$ o número de elementos na matriz esparsa $B$.
    \subsubsection{Complexidade de \texttt{matrix\_addition\_hash}}
    Nessa operação, criamos uma matriz resultado (custo constante), então percorremos o vetor de espalhamento para as duas matrizes que serão somadas, $A$ e $B$. Para cada elemento do vetor, percorremos o bucket associado. O custo esperado de percorrer o vetor de espalhamento, mesmo se também percorremos os buckets de cada elemento, é linear em $K$. Como percorremos $A$, e depois $B$, o custo é $T(k) \in O(Ka + Kb)$, sendo $Ka$ o número de elementos na matriz esparsa $A$ e $Kb$ o número de elementos na matriz esparsa $B$.
    \subsubsection{Complexidade de \texttt{matrix\_scalar\_multiplication\_hash}}
    Nessa operação, criamos uma matriz resultado (custo constante), então percorremos o vetor de espalhamento, e para cada elemento do vetor, percorremos o bucket associado. Como já discutido antes, o custo esperado de percorrer o vetor de espalhamento, mesmo se também percorremos os buckets de cada elemento, $T(k) \in O(k)$.
    
    \subsubsection{Complexidade de \texttt{transpose\_hash}}
    Essa operação tem somente uma linha, que é uma mudança de um valor booleano para seu oposto. Isso tem custo constante. A forma como a transposição é de fato feita é pela mudança entre linhas e colunas nas outras operações. Portanto $T(k) \in \Theta(1)$.
    
    \subsection{Estrutura 2 - AVL}
    \begin{table}[h]
        \centering
        \begin{tabular}{|l|c|}
            \hline
            \multicolumn{1}{|c|}{\textbf{Função}} & \multicolumn{1}{c|}{\textbf{Complexidade de Tempo}} \\ \hline
            \texttt{avl\_status\_string} & $O(1)$ \\ \hline
            \texttt{create\_matrix\_avl} & $O(1)$ \\ \hline
            \texttt{transpose\_avl} & $O(1)$ \\ \hline
            \texttt{\_allocation\_fail} & $O(1)$ \\ \hline
            \texttt{\_validate\_matrix} & $O(1)$ \\ \hline
            \texttt{\_validate\_indices} & $O(1)$ \\ \hline
            \texttt{\_validate\_same\_dimensions} & $O(1)$ \\ \hline
            \texttt{\_height\_i} & $O(1)$ \\ \hline
            \texttt{\_height\_o} & $O(1)$ \\ \hline
            \texttt{\_balance\_factor\_i} & $O(1)$ \\ \hline
            \texttt{\_balance\_factor\_o} & $O(1)$ \\ \hline
            \texttt{\_max} & $O(1)$ \\ \hline
            \texttt{\_left\_rotate\_i} & $O(1)$ \\ \hline
            \texttt{\_right\_rotate\_i} & $O(1)$ \\ \hline
            \texttt{\_left\_rotate\_o} & $O(1)$ \\ \hline
            \texttt{\_right\_rotate\_o} & $O(1)$ \\ \hline
        \end{tabular}
        \caption{Funções com complexidade de tempo trivial da estrutura AVL.}
    \end{table}

    \begin{lemma}\label{altura_AVL}
    Sendo $h$ a altura de uma árvore AVL e $n$ seu número de nós, $h \in O(\log n)$.
    \end{lemma}

    \begin{proof}
    A árvore AVL tem a garantia de que $|h_e - h_d| \le 1$ para todo nó. Seja $N(h)$ o número mínimo de nós de uma árvore AVL. Temos que, a árvore menos cheia possível que ainda respeita a condição de AVL deve respeitar:

    \[
    N(h) =
    \begin{cases}
        1 & \text{se } h = 1\\
        2 & \text{se } h = 2\\
        1 + N(h-1) + N(h-2) & \text{se } h > 2
    \end{cases}\]

    \noindent Vamos provar por indução que $N \in \Omega(2^{\frac{h}{2}})$. Escolhendo $c = \frac{\sqrt{2}}{2}$, caso base:

    \[1 \ge c\cdot \sqrt{2} \quad \land \quad 2 \ge c\cdot 2\]

    \noindent Hipótese Indutiva: 
    \[\forall i (1 \le i < h) \rightarrow c\cdot 2^{\frac{i}{2}} \le N(i)\]
    \noindent Passo Indutivo: 

    \begin{align*}
        N(h) & = 1 + N(h-1) + N(h-2) \\
             & \ge 1 + c\cdot 2^{\frac{h-1}{2}} + c\cdot 2^{\frac{h-2}{2}} \\
             & = 1 + c\cdot 2^{\frac{h}{2}} \left(\frac{1}{\sqrt{2}} + \frac{1}{2}\right) \\
             & \ge c\cdot 2^{\frac{h}{2}}.
    \end{align*}

    \noindent Assim, $N(h) \ge c\cdot 2^{\frac{h}{2}}$ e, dado que cada árvore AVL com altura $h$ possui pelo menos $N(h)$ nós, temos $n \ge N(h)$. Logo:

    \[n \in \Omega(2^{\frac{h}{2}}) \implies h \in O(\log n)\]

    \end{proof}

    \subsubsection{Complexidade de espaço de \texttt{AVLMatrix}}

    Os campos de tamanho variável de \texttt{AVLMatrix} são \texttt{main\_root} e \texttt{transposed\_root}. Ambos são árvores de nós "externos", onde cada nó contem árvores de nós "internos", que contem o dado armazenado. O somatório da quantidade de nós internos deve ser $k$, uma vez que só são armazenados elementos não nulos. No pior caso, cada nó "externo" guarda somente um nó "interno", gerando assim $k$ nós internos e externos para \texttt{main\_root} e $k$ nós internos e externos para \texttt{transposed\_root}. Assim, temos um gasto de memória proporcional à constante referente ao tamanho dos nós multiplicada por dois para cada um dos $k$ nós. Sendo assim, a estrutura gasta, trivialmente, $M(k) \in O(k)$ memória para o número de dados não nulos na matriz esparsa $k$.

    \subsubsection{Complexidade de \texttt{\_find\_node\_i} e \texttt{\_find\_node\_o}}

    A função faz somente um caminho da raiz até a folha, uma vez que, para cada nó, visita somente ou a sub-árvore esquerda ou a sub-árvore direita, sempre. Em cada chamada, é executado esforço computacional constante. Dessa forma, o algoritmo possui complexidade $T(h) \in O(h) \implies T(n) \in O(\log n)$, pelo lema \ref{altura_AVL}, sendo $n$ o número de nós e $h$ a altura.\\ \qed

    \subsubsection{Complexidade de \texttt{\_insert\_i} e \texttt{\_insert\_o}}

    A função faz somente um caminho da raiz até a folha, uma vez que, para cada nó, visita somente ou a sub-árvore esquerda ou a sub-árvore direita, sempre. Em cada chamada, é executado esforço computacional constante. Dessa forma, o algoritmo possui complexidade $T(h) \in O(h) \implies T(n) \in O(\log n)$, pelo lema \ref{altura_AVL}, sendo $n$ o número de nós e $h$ a altura.\\ \qed

    \subsubsection{Complexidade de \texttt{\_find\_max\_i} e \texttt{\_find\_max\_o}}

    A função faz somente um caminho da raiz até a folha, indo sempre pela sub-árvore direita. Assim, esse caminho está limitado pela altura da árvore, e esforço computacional constante é executado em cada chamada. Segue que o algoritmo possui complexidade $T(h) \in O(h) \implies T(n) \in O(\log n)$, pelo lema \ref{altura_AVL}, sendo $n$ o número de nós e $h$ a altura.\\ \qed

    \subsubsection{Complexidade de \texttt{\_remove\_i} e \texttt{\_remove\_o}}

    A função faz somente um caminho da raiz até o nó, uma vez que, para cada nó, visita somente ou a sub-árvore esquerda ou a sub-árvore direita, sempre. Em cada chamada não base, é executado esforço computacional constante. Dessa forma, o algoritmo possui complexidade $T(h) = T_r(h) + T_b(h)$. $T_r(h) \in O(h) \implies T_r(n) \in O(\log n)$, e o caso base de remoção de nó com dois filhos faz esforço constante, uma chamada para \texttt{\_find\_max\_*}, que faz esforço computacional linear na altura, e uma chamada para a própria função, com a garantia de cair num caso base de esforço computacional constante -- uma vez que o nó a ser removido é um nó folha. Dessa forma, ambas chamadas são realizadas em sub-árvores da árvore original, sendo limitadas superiormente pela altura da árvore total. Assim, $T_b(h) \in O(h) \implies T_b(n) \in O(\log n)$, o que leva a $T(h) \in O(h) \implies T(n) \in O(\log n)$.\\ \qed

    \subsubsection{Complexidade de \texttt{\_free\_i\_tree} e \texttt{\_free\_o\_tree}}

    Cada nó da árvore é visitado recursivamente, realizando esforço computacional constante em cada chamada no caso "inner". Assim, $T(n) \in O(n)$. No caso "outer", cada nó é visitado recursivamente, e em cada chamada recursiva, cada nó da árvore interna contida no nó externo é visitada pela função "inner". Dessa forma, $T(n) \in O(n)$, com a diferença que $n$ é o número total de nós "inner" em uma árvore "outer", ou seja, o número total de elementos que a estrutura armazena.\\ \qed

    \subsubsection{Complexidade de \texttt{\_clone\_i\_tree} e \texttt{\_clone\_o\_tree}}

    Cada nó da árvore é visitado recursivamente, realizando esforço computacional constante em cada chamada no caso "inner". Assim, $T(n) \in O(n)$. No caso "outer", cada nó é visitado recursivamente, e em cada chamada recursiva, cada nó da árvore interna contida no nó externo é visitada pela função "inner". Dessa forma, $T(n) \in O(n)$, com a diferença que $n$ é o número total de nós "inner" em uma árvore "outer", ou seja, o número total de elementos que a estrutura armazena. Essa função cria uma nova árvore que é uma exata cópia da árvore anterior, gastando assim $M(n) \in O(n)$ de memória.\\ \qed

    \subsubsection{Complexidade de \texttt{\_copy\_matrix}}

    Essa função faz esforço computacional constante e invoca duas funções de esforço computacional linear na quantidade de elementos duas vezes cada. Assim, a função possui complexidade $T(n) \in O(n)$.

    \subsubsection{Complexidade de \texttt{\_scalar\_multiply\_i\_tree} e \texttt{\_scalar\_multiply\_o\_tree}}

    Cada nó da árvore é visitado recursivamente, realizando esforço computacional constante em cada chamada no caso "inner". Assim, $T(n) \in O(n)$. No caso "outer", cada nó é visitado recursivamente, e em cada chamada recursiva, cada nó da árvore interna contida no nó externo é visitada pela função "inner". Dessa forma, $T(n) \in O(n)$, com a diferença que $n$ é o número total de nós "inner" em uma árvore "outer", ou seja, o número total de elementos que a estrutura armazena.\\ \qed

    \subsubsection{Complexidade de \texttt{\_copy\_i} e \texttt{\_copy\_o}}

    Cada nó da árvore é visitado recursivamente, realizando esforço computacional constante em cada chamada no caso "inner". Assim, $T(n) \in O(n)$. No caso "outer", cada nó é visitado recursivamente, e em cada chamada recursiva, cada nó da árvore interna contida no nó externo é visitada pela função "inner". Dessa forma, $T(n) \in O(n)$, com a diferença que $n$ é o número total de nós "inner" em uma árvore "outer", ou seja, o número total de elementos que a estrutura armazena.\\ \qed

    \subsubsection{Complexidade de \texttt{\_matmul\_i\_accumulate}}

    Cada nó da árvore é visitado recursivamente. A árvore interna representa exatamente uma coluna da matriz, e cada nó corresponde a um elemento não nulo. Denominamos $l$ a quantidade de elementos não nulos na dada coluna da matriz, que é a quantidade de elementos na árvore. Para cada chamada, o algoritmo chama dois algoritmos que serão provados como logarítmicos na quantidade de nós da referida árvore, além de realizar esforço computacional constante. Sendo assim, esse algoritmo possui complexidade $T(l,k_C) \in O(l\cdot \log k_C)$.

    \subsubsection{Complexidade de \texttt{get\_element\_avl}}
    O algoritmo faz duas chamadas para algoritmos logarítmicos na quantidade de nós da árvore, além de chamadas de tempo constante. Assim, garantimos complexidade $T(k) \in O(1 + \log k)$.

    \subsubsection{Complexidade de \texttt{insert\_element\_avl}}
    O algoritmo faz oito chamadas para algoritmos logarítmicos na quantidade de nós da árvore, além de chamadas de tempo constante. Assim, garantimos complexidade $T(k) \in O(1 + \log k)$.

    \subsubsection{Complexidade de \texttt{delete\_element\_avl}}
    O algoritmo faz sete chamadas para algoritmos logarítmicos na quantidade de nós da árvore, além de chamadas de tempo constante. Assim, garantimos complexidade $T(k) \in O(1 + \log k)$.

    \subsubsection{Complexidade de \texttt{transpose\_avl}}
    O algoritmo faz apenas operações de tempo constante, garantindo $T(k) \in O(1)$.

    \subsubsection{Complexidade de \texttt{scalar\_mul\_avl}}
    O algoritmo faz no máximo uma quantidade constante de chamadas parra algoritmos lineares na quantidade de nós da árvore, além de chamadas de tempo constante. Assim, garantimos complexidade $T(k) \in O(1 + k)$.

    \subsubsection{Complexidade de \texttt{sum\_avl}}
    O algoritmo chama um algoritmo linear na quantidade de nós para a matriz $B$, fazendo esforço proporcional a $k_B$. Ele também faz uma chamada a um algoritmo linear para a matriz $A$, fazendo esforço proporcional a $k_A$ e alocando $k_A$ de memória (o que ainda garante uso de memória $O(k)$). Por último, para cada elemento $k_A$ da matriz esparsa linearizada, são chamados dois algoritmos que fazem esforço logarítmico na matriz $C$, com $k$ elementos. Assim, temos esforço computacional proporcional a $k_A + k_B + k_A \cdot \log k < k_A + k_B \cdot \log k + k_A \cdot \log k$ o que implica em $T(k_A,k_B,k) \in O(1 + (k_A + k_B)\log k)$.

    \subsubsection{Complexidade de \texttt{matrix\_mul\_avl}}

    O algoritmo começa com chamadas a \texttt{\_free\_o\_tree}, o que pode ser desconsiderado, já que faz esforço computacional constante se a matriz $C$ estiver vazia por estipulação. A chamada dessa função é uma mera precaução. O algoritmo reserva espaço proporcional a $3k$, o que não viola o requisito de memória $O(k)$. Depois, um algoritmo linear é chamado em $A$, fazendo esforço computacional proporcional a $k_A$. Então, para cada elemento de um vetor de tamanho $k_A$, é feito esforço computacional $\log(r_B)$, sendo $r_B$ a quantidade de linhas não-nulas da matriz, ainda tendo que $r_B < k_N$. Então, para cada elemento $k_A$ é feito no máximo esforço $c_B\log(k_C)$, sendo $c_B$ a quantidade de colunas não-nulas em $B$. Denotando por $m_B = \max\{c_B , r_B\}$, a complexidade total é proporcional a $k_A + k_A \log r_B + k_A c_B \log k_C < k_A \log m_B \log k_C +  k_A \log m_B \log k_C +  k_A \log m_B \log k_C = k_A \log k_B \log k_C < 3 k_A k_B \log k_C \in O(1 + k_A k_B (1+ \log (1+k_C)))$. De fato, uma análise assintótica mais justa para o algoritmo é $O(1 + k_A + k_A \cdot \log (1+ m_B) \log (1 + k_C) )$

    \subsubsection{Complexidade de \texttt{create\_matrix\_avl}}

    O algoritmo faz apenas operações de tempo constante, garantindo $T(k) \in O(1)$.

    \subsubsection{Complexidade de \texttt{free\_matrix\_avl}}

    O algoritmo faz duas chamadas para algoritmos lineares na quantidade de nós e faz esforço constante, garantindo $T(k) \in O(k)$.

    \section{Análise Experimental}

    \subsection{Dados}
    \subsubsection{Resultados de Tamanho}
    \begin{center}
    \label{tab:tamanhos}
    \resizebox{0.9\linewidth}{!}{\input{tables_sizes.tex}}
    \captionof{table}{Tamanhos (bytes) para Dense, AVL e Hash.}
    \end{center}

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.45\textwidth]{memory_vs_sparsity_n100.png}
        \includegraphics[width=0.45\textwidth]{memory_vs_sparsity_n1000.png}
        \caption{Memória vs. esparsidade (log-log) para $n=100$ e $n=1000$.}
    \end{figure}

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.45\textwidth]{memory_vs_sparsity_n10000.png}
        \includegraphics[width=0.45\textwidth]{memory_vs_sparsity_n100000.png}
        \caption{Memória vs. esparsidade (log-log) para $n=10{,}000$ e $n=100{,}000$.}
    \end{figure}

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.45\textwidth]{memory_vs_sparsity_n1000000.png}
        \caption{Memória vs. esparsidade (log-log) para $n=1{,}000{,}000$.}
    \end{figure}

    \subsubsection{Resultados de Tempo}
    \begin{center}
        \begin{minipage}{0.48\textwidth}
            \centering
            \resizebox{\linewidth}{!}{\input{tables_time_get.tex}}
            \captionof{table}{Temporização de \texttt{get}}
        \end{minipage}\hfill
        \begin{minipage}{0.48\textwidth}
            \centering
            \resizebox{\linewidth}{!}{\input{tables_time_set.tex}}
            \captionof{table}{Temporização de \texttt{set}}
        \end{minipage}
    \end{center}

    \begin{center}
        \begin{minipage}{0.48\textwidth}
            \centering
            \resizebox{\linewidth}{!}{\input{tables_time_trans.tex}}
            \captionof{table}{Temporização de \texttt{transpose}}
        \end{minipage}\hfill
        \begin{minipage}{0.48\textwidth}
            \centering
            \resizebox{\linewidth}{!}{\input{tables_time_scalar.tex}}
            \captionof{table}{Temporização de \texttt{scalar\_mul}}
        \end{minipage}
    \end{center}

    \begin{center}
        \begin{minipage}{0.48\textwidth}
            \centering
            \resizebox{\linewidth}{!}{\input{tables_time_sum.tex}}
            \captionof{table}{Temporização de \texttt{sum}}
        \end{minipage}\hfill
        \begin{minipage}{0.48\textwidth}
            \centering
            \resizebox{\linewidth}{!}{\input{tables_time_mul.tex}}
            \captionof{table}{Temporização de \texttt{matmul}}
        \end{minipage}
    \end{center}

    \subsection{Interpretação}

    Através da análise dos dados experimentais e da compreensão da diferença entre um pior caso assintótico garantido e um pior caso assintótico esperado, é possível tirar algumas conclusões:

    Primeiro, pelos dados é possível observar que o uso de matrizes densas, em geral, é mais vantajoso para pequenos tamanhos. Se é possível armazenar a matriz por completo, as operações são ordens de grandeza mais rápidas, e pensando em memória, o \emph{overhead} gerado pelas implementações propostas pode ser maior em tamanhos pequenos. Porém, para grandes matrizes, a situação é a inversa -- os dados experimentais param em $n = 10^3$ pois o tempo para a execução das multiplicações de matrizes densas era impraticável, e para $n = 10^4$ é virtualmente impossível armazenar matrizes densas em hardware de consumidor. Já para as implementações de AVL e Hash, o baixo grau de esparsidade especificado pelo enunciado permite as operações em tempo baixíssimo, com uso de memória totalmente viável. Mesmo em graus de esparsidade ligeiramente mais altos, a dependência assintótica da memória apenas nos dados efetivamente armazenados torna viável o uso dessas estruturas. Dessa forma, pode-se concluir que o uso mais vantajoso desse tipo de representação de matriz esparsa é para armazenar e operar em cima de dados matriciais de grandes ordens -- com a implementação densa, existem aplicações simplesmente impossíveis, que tornam-se viáveis com o uso das estruturas esparsas.

    Entre as duas estruturas propostas, existem motivos para usar uma \emph{versus} a outra. Em todas as operações com exceção da \texttt{matmul}, a estrutura baseada em Hash se mostrou experimentalmente mais rápida, mas suas garantias são fracas -- estatisticamente, ela respeita os limites impostos, mas os limites para o pior caso possível são altíssimos comparados à implementação por AVL. Assim, em aplicações de tempo real com necessidade de garantias fortes, é péssima prática utilizar uma estrutura que tem sua performance totalmente dependente na distribuição dos dados de entrada.

    \begin{thebibliography}{1}
        \bibitem{gilberg2004data}
        GILBERG, R.~F.; FOROUZAN, B.~A. \textit{Data Structures: A Pseudocode Approach with C}. Cengage Learning, 2004.
    \end{thebibliography}
    
\end{document}
